{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from models.transformer import Transformer, create_look_ahead_mask\n",
    "from dataset.dataset_extracted import ExtractedFeatureDataset\n",
    "import sentencepiece as spm\n",
    "import transforms\n",
    "from evaluate import BeamSearchCaptioner\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus_file = '../MVAD/corpus_M-VAD_{}.txt'.format(flag)\n",
    "tokenizer_file = 'tokenizer.model'\n",
    "model_weight_file = '../checkpoint/20190815013812/100'\n",
    "inp_max_seq_length = 50\n",
    "tar_max_seq_length = 50\n",
    "tar_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "encoder_num_layers = 6\n",
    "decoder_num_layers = 6\n",
    "dff = 2048\n",
    "dropout = 0.1\n",
    "max_seq_length = 80  # For positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_path = '../MVAD/I3D_rgb_kinetics/{}'.format(flag)\n",
    "with open('../MVAD/{}_fine'.format(flag)) as f:\n",
    "    files = f.readlines()\n",
    "    feature_files = list(map(lambda file: os.path.join(test_feature_path, str.strip(file) + '.npy'), files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_transform = transforms.Compose([\n",
    "    transforms.FeaturePadding(inp_max_seq_length)\n",
    "])\n",
    "caption_transform = transforms.Compose([\n",
    "    transforms.CaptionPadding(tar_max_seq_length, sp.PieceToId('<PAD>'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ExtractedFeatureDataset(None, test_corpus_file, inp_max_seq_length, tar_max_seq_length, sp, feature_transform=feature_transform, caption_transform=caption_transform, feature_files=feature_files)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (linear): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (pe): PositionalEncoder()\n",
       "  (encoder): TransformerEncoder(\n",
       "    (encoder_layers): ModuleList(\n",
       "      (encoder_layer1): TransformerEncoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (an1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (an2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer2): TransformerEncoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (an1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (an2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer3): TransformerEncoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (an1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (an2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer4): TransformerEncoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (an1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (an2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer5): TransformerEncoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (an1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (an2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer6): TransformerEncoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (an1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (an2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (word_embedding): Embedding(5000, 512)\n",
       "  (decoder): TransformerDecoder(\n",
       "    (decoder_layers): ModuleList(\n",
       "      (decoder_layer1): TransformerDecoderLayer(\n",
       "        (mhas1): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mhas2): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (an1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (an2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (an3): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (decoder_layer2): TransformerDecoderLayer(\n",
       "        (mhas1): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mhas2): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (an1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (an2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (an3): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (decoder_layer3): TransformerDecoderLayer(\n",
       "        (mhas1): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mhas2): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (an1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (an2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (an3): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (decoder_layer4): TransformerDecoderLayer(\n",
       "        (mhas1): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mhas2): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (an1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (an2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (an3): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (decoder_layer5): TransformerDecoderLayer(\n",
       "        (mhas1): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mhas2): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (an1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (an2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (an3): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (decoder_layer6): TransformerDecoderLayer(\n",
       "        (mhas1): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mhas2): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (an1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (an2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (an3): AddNorm(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (layernorm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (emb2voc): Linear(in_features=512, out_features=5000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(tar_vocab_size, d_model, num_heads, encoder_num_layers, decoder_num_layers, dff, dropout, max_seq_length)\n",
    "state_dict = torch.load(model_weight_file, map_location='cpu')\n",
    "model.load_state_dict(state_dict['model_state_dict'], strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = dataiter.next()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.metrics.bleu import get_moses_multi_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = caption.squeeze(0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 91,\n",
       " 1842,\n",
       " 15,\n",
       " 358,\n",
       " 293,\n",
       " 6,\n",
       " 17,\n",
       " 5,\n",
       " 3064,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_moses_multi_bleu(sp.decode_ids(caption), sp.decode_ids(caption), lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video: ('KARATE_KID_DVS1138.avi',)\n",
      "caption origin: Beaming, he returns the gesture.\n",
      "step: 2, end_nodes: 0\n",
      "step: 3, end_nodes: 0\n",
      "step: 4, end_nodes: 0\n",
      "step: 5, end_nodes: 0\n",
      "step: 6, end_nodes: 0\n",
      "step: 7, end_nodes: 0\n",
      "step: 8, end_nodes: 0\n",
      "step: 9, end_nodes: 0\n",
      "step: 10, end_nodes: 1\n",
      "step: 11, end_nodes: 2\n",
      "step: 12, end_nodes: 3\n",
      "step: 13, end_nodes: 4\n",
      "step: 14, end_nodes: 5\n",
      "step: 15, end_nodes: 6\n",
      "step: 16, end_nodes: 6\n",
      "step: 17, end_nodes: 6\n",
      "step: 18, end_nodes: 8\n",
      "step: 19, end_nodes: 9\n",
      "step: 20, end_nodes: 9\n",
      "caption predict: Beaming, he returns the gesture.\n",
      "\n",
      "video: ('SALT_DVS60.avi',)\n",
      "caption origin: The chopper arrives at a fortress.\n",
      "step: 2, end_nodes: 0\n",
      "step: 3, end_nodes: 0\n",
      "step: 4, end_nodes: 0\n",
      "step: 5, end_nodes: 0\n",
      "step: 6, end_nodes: 0\n",
      "step: 7, end_nodes: 0\n",
      "step: 8, end_nodes: 0\n",
      "step: 9, end_nodes: 0\n",
      "step: 10, end_nodes: 0\n",
      "step: 11, end_nodes: 1\n",
      "step: 12, end_nodes: 1\n",
      "step: 13, end_nodes: 2\n",
      "step: 14, end_nodes: 3\n",
      "step: 15, end_nodes: 3\n",
      "step: 16, end_nodes: 5\n",
      "step: 17, end_nodes: 6\n",
      "step: 18, end_nodes: 6\n",
      "step: 19, end_nodes: 6\n",
      "step: 20, end_nodes: 6\n",
      "step: 21, end_nodes: 8\n",
      "step: 22, end_nodes: 8\n",
      "step: 23, end_nodes: 8\n",
      "step: 24, end_nodes: 8\n",
      "step: 25, end_nodes: 8\n",
      "step: 26, end_nodes: 8\n",
      "step: 27, end_nodes: 8\n",
      "step: 28, end_nodes: 8\n",
      "step: 29, end_nodes: 8\n",
      "step: 30, end_nodes: 8\n",
      "step: 31, end_nodes: 8\n",
      "step: 32, end_nodes: 8\n",
      "step: 33, end_nodes: 8\n",
      "step: 34, end_nodes: 8\n",
      "step: 35, end_nodes: 8\n",
      "step: 36, end_nodes: 8\n",
      "step: 37, end_nodes: 8\n",
      "step: 38, end_nodes: 8\n",
      "step: 39, end_nodes: 8\n",
      "step: 40, end_nodes: 8\n",
      "step: 41, end_nodes: 8\n",
      "step: 42, end_nodes: 8\n",
      "step: 43, end_nodes: 8\n",
      "step: 44, end_nodes: 8\n",
      "step: 45, end_nodes: 8\n",
      "step: 46, end_nodes: 8\n",
      "step: 47, end_nodes: 8\n",
      "step: 48, end_nodes: 8\n",
      "step: 49, end_nodes: 8\n",
      "step: 50, end_nodes: 8\n",
      "caption predict: The chopper arrives at a fortres.\n",
      "\n",
      "video: ('IN_TIME_DVS860.avi',)\n",
      "caption origin: He glances at the digital map and data tables as SOMEONE leads them past.\n",
      "step: 2, end_nodes: 0\n",
      "step: 3, end_nodes: 0\n",
      "step: 4, end_nodes: 0\n",
      "step: 5, end_nodes: 0\n",
      "step: 6, end_nodes: 0\n",
      "step: 7, end_nodes: 0\n",
      "step: 8, end_nodes: 0\n",
      "step: 9, end_nodes: 0\n",
      "step: 10, end_nodes: 0\n",
      "step: 11, end_nodes: 0\n",
      "step: 12, end_nodes: 0\n",
      "step: 13, end_nodes: 0\n",
      "step: 14, end_nodes: 0\n",
      "step: 15, end_nodes: 0\n",
      "step: 16, end_nodes: 0\n",
      "step: 17, end_nodes: 0\n",
      "step: 18, end_nodes: 0\n",
      "step: 19, end_nodes: 0\n",
      "step: 20, end_nodes: 0\n",
      "step: 21, end_nodes: 0\n",
      "step: 22, end_nodes: 0\n",
      "step: 23, end_nodes: 0\n",
      "step: 24, end_nodes: 0\n",
      "step: 25, end_nodes: 0\n",
      "step: 26, end_nodes: 3\n",
      "step: 27, end_nodes: 3\n",
      "step: 28, end_nodes: 3\n",
      "step: 29, end_nodes: 4\n",
      "step: 30, end_nodes: 4\n",
      "step: 31, end_nodes: 4\n",
      "step: 32, end_nodes: 4\n",
      "step: 33, end_nodes: 5\n",
      "step: 34, end_nodes: 5\n",
      "step: 35, end_nodes: 5\n",
      "step: 36, end_nodes: 6\n",
      "step: 37, end_nodes: 7\n",
      "step: 38, end_nodes: 8\n",
      "step: 39, end_nodes: 8\n",
      "step: 40, end_nodes: 8\n",
      "step: 41, end_nodes: 8\n",
      "step: 42, end_nodes: 9\n",
      "step: 43, end_nodes: 9\n",
      "step: 44, end_nodes: 9\n",
      "caption predict: He glances at the digital map and data tables as SOMEONE leads them past. Now the feet of the facility.\n",
      "\n",
      "video: ('IN_TIME_DVS950.avi',)\n",
      "caption origin:  ⁇ 0 seconds. He glances over his shoulder.\n",
      "step: 2, end_nodes: 0\n",
      "step: 3, end_nodes: 0\n",
      "step: 4, end_nodes: 0\n",
      "step: 5, end_nodes: 0\n",
      "step: 6, end_nodes: 0\n",
      "step: 7, end_nodes: 0\n",
      "step: 8, end_nodes: 0\n",
      "step: 9, end_nodes: 0\n",
      "step: 10, end_nodes: 0\n",
      "step: 11, end_nodes: 0\n",
      "step: 12, end_nodes: 0\n",
      "step: 13, end_nodes: 0\n",
      "step: 14, end_nodes: 0\n",
      "step: 15, end_nodes: 2\n",
      "step: 16, end_nodes: 2\n",
      "step: 17, end_nodes: 2\n",
      "step: 18, end_nodes: 2\n",
      "step: 19, end_nodes: 2\n",
      "step: 20, end_nodes: 4\n",
      "step: 21, end_nodes: 5\n",
      "step: 22, end_nodes: 5\n",
      "step: 23, end_nodes: 5\n",
      "step: 24, end_nodes: 5\n",
      "step: 25, end_nodes: 5\n",
      "step: 26, end_nodes: 5\n",
      "step: 27, end_nodes: 5\n",
      "step: 28, end_nodes: 5\n",
      "step: 29, end_nodes: 5\n",
      "step: 30, end_nodes: 5\n",
      "step: 31, end_nodes: 5\n",
      "step: 32, end_nodes: 5\n",
      "step: 33, end_nodes: 6\n",
      "step: 34, end_nodes: 6\n",
      "step: 35, end_nodes: 6\n",
      "step: 36, end_nodes: 6\n",
      "step: 37, end_nodes: 6\n",
      "step: 38, end_nodes: 6\n",
      "step: 39, end_nodes: 6\n",
      "step: 40, end_nodes: 6\n",
      "step: 41, end_nodes: 6\n",
      "step: 42, end_nodes: 7\n",
      "step: 43, end_nodes: 7\n",
      "step: 44, end_nodes: 8\n",
      "step: 45, end_nodes: 8\n",
      "step: 46, end_nodes: 9\n",
      "step: 47, end_nodes: 9\n",
      "caption predict:  ⁇ 0 seconds. He glances over his shoulder.\n",
      "\n",
      "video: ('HUGO_DVS203.avi',)\n",
      "caption origin: SOMEONE stands in front of the toyshop and stares at SOMEONE.\n",
      "step: 2, end_nodes: 0\n",
      "step: 3, end_nodes: 0\n",
      "step: 4, end_nodes: 0\n",
      "step: 5, end_nodes: 0\n",
      "step: 6, end_nodes: 0\n",
      "step: 7, end_nodes: 0\n",
      "step: 8, end_nodes: 0\n",
      "step: 9, end_nodes: 0\n",
      "step: 10, end_nodes: 0\n",
      "step: 11, end_nodes: 0\n",
      "step: 12, end_nodes: 0\n",
      "step: 13, end_nodes: 0\n",
      "step: 14, end_nodes: 0\n",
      "step: 15, end_nodes: 0\n",
      "step: 16, end_nodes: 0\n",
      "step: 17, end_nodes: 0\n",
      "step: 18, end_nodes: 0\n",
      "step: 19, end_nodes: 0\n",
      "step: 20, end_nodes: 0\n",
      "step: 21, end_nodes: 0\n",
      "step: 22, end_nodes: 0\n",
      "step: 23, end_nodes: 1\n",
      "step: 24, end_nodes: 1\n",
      "step: 25, end_nodes: 1\n",
      "step: 26, end_nodes: 1\n",
      "step: 27, end_nodes: 1\n",
      "step: 28, end_nodes: 1\n",
      "step: 29, end_nodes: 1\n",
      "step: 30, end_nodes: 1\n",
      "step: 31, end_nodes: 1\n",
      "step: 32, end_nodes: 1\n",
      "step: 33, end_nodes: 1\n",
      "step: 34, end_nodes: 2\n",
      "step: 35, end_nodes: 2\n",
      "step: 36, end_nodes: 2\n",
      "step: 37, end_nodes: 3\n",
      "step: 38, end_nodes: 4\n",
      "step: 39, end_nodes: 4\n",
      "step: 40, end_nodes: 5\n",
      "step: 41, end_nodes: 5\n",
      "step: 42, end_nodes: 5\n",
      "step: 43, end_nodes: 5\n",
      "step: 44, end_nodes: 5\n",
      "step: 45, end_nodes: 5\n",
      "step: 46, end_nodes: 5\n",
      "step: 47, end_nodes: 6\n",
      "step: 48, end_nodes: 6\n",
      "step: 49, end_nodes: 8\n",
      "step: 50, end_nodes: 8\n",
      "caption predict: SOMEONE crosses her aunt sitting her father dinner and SOMEONE gives her a drag aims her gaze.\n",
      "\n",
      "video: ('ABRAHAM_LINCOLN_VAMPIRE_HUNTER_DVS358.avi',)\n",
      "caption origin: He slams the casket's lid.\n",
      "step: 2, end_nodes: 0\n",
      "step: 3, end_nodes: 0\n",
      "step: 4, end_nodes: 0\n",
      "step: 5, end_nodes: 0\n",
      "step: 6, end_nodes: 0\n",
      "step: 7, end_nodes: 0\n",
      "step: 8, end_nodes: 0\n",
      "step: 9, end_nodes: 0\n",
      "step: 10, end_nodes: 0\n",
      "step: 11, end_nodes: 0\n",
      "step: 12, end_nodes: 0\n",
      "step: 13, end_nodes: 0\n",
      "step: 14, end_nodes: 1\n",
      "step: 15, end_nodes: 1\n",
      "step: 16, end_nodes: 1\n",
      "step: 17, end_nodes: 1\n",
      "step: 18, end_nodes: 1\n",
      "step: 19, end_nodes: 1\n",
      "step: 20, end_nodes: 1\n",
      "step: 21, end_nodes: 1\n",
      "step: 22, end_nodes: 1\n",
      "step: 23, end_nodes: 1\n",
      "step: 24, end_nodes: 1\n",
      "step: 25, end_nodes: 1\n",
      "step: 26, end_nodes: 1\n",
      "step: 27, end_nodes: 1\n",
      "step: 28, end_nodes: 1\n",
      "step: 29, end_nodes: 3\n",
      "step: 30, end_nodes: 3\n",
      "step: 31, end_nodes: 3\n",
      "step: 32, end_nodes: 3\n",
      "step: 33, end_nodes: 3\n",
      "step: 34, end_nodes: 3\n",
      "step: 35, end_nodes: 4\n",
      "step: 36, end_nodes: 5\n",
      "step: 37, end_nodes: 5\n",
      "step: 38, end_nodes: 5\n",
      "step: 39, end_nodes: 5\n",
      "step: 40, end_nodes: 5\n",
      "step: 41, end_nodes: 5\n",
      "step: 42, end_nodes: 5\n",
      "step: 43, end_nodes: 5\n",
      "step: 44, end_nodes: 5\n",
      "step: 45, end_nodes: 5\n",
      "step: 46, end_nodes: 5\n",
      "step: 47, end_nodes: 5\n",
      "step: 48, end_nodes: 6\n",
      "step: 49, end_nodes: 8\n",
      "step: 50, end_nodes: 8\n",
      "caption predict: Now SOMEONE and SOMEONE box her father aims her gaze.\n",
      "\n",
      "video: ('SUPER_8_DVS499.avi',)\n",
      "caption origin: A car speeds toward the bus.\n",
      "step: 2, end_nodes: 0\n",
      "step: 3, end_nodes: 0\n",
      "step: 4, end_nodes: 0\n",
      "step: 5, end_nodes: 0\n",
      "step: 6, end_nodes: 0\n",
      "step: 7, end_nodes: 0\n",
      "step: 8, end_nodes: 0\n",
      "step: 9, end_nodes: 0\n",
      "step: 10, end_nodes: 0\n",
      "step: 11, end_nodes: 0\n",
      "step: 12, end_nodes: 0\n",
      "step: 13, end_nodes: 1\n",
      "step: 14, end_nodes: 1\n",
      "step: 15, end_nodes: 2\n",
      "step: 16, end_nodes: 2\n",
      "step: 17, end_nodes: 2\n",
      "step: 18, end_nodes: 2\n",
      "step: 19, end_nodes: 2\n",
      "step: 20, end_nodes: 2\n",
      "step: 21, end_nodes: 2\n",
      "step: 22, end_nodes: 2\n",
      "step: 23, end_nodes: 2\n",
      "step: 24, end_nodes: 2\n",
      "step: 25, end_nodes: 2\n",
      "step: 26, end_nodes: 3\n",
      "step: 27, end_nodes: 3\n",
      "step: 28, end_nodes: 3\n",
      "step: 29, end_nodes: 3\n",
      "step: 30, end_nodes: 3\n",
      "step: 31, end_nodes: 3\n",
      "step: 32, end_nodes: 3\n",
      "step: 33, end_nodes: 3\n",
      "step: 34, end_nodes: 3\n",
      "step: 35, end_nodes: 3\n",
      "step: 36, end_nodes: 3\n",
      "step: 37, end_nodes: 3\n",
      "step: 38, end_nodes: 3\n",
      "step: 39, end_nodes: 3\n",
      "step: 40, end_nodes: 3\n",
      "step: 41, end_nodes: 3\n",
      "step: 42, end_nodes: 3\n",
      "step: 43, end_nodes: 3\n",
      "step: 44, end_nodes: 3\n",
      "step: 45, end_nodes: 3\n",
      "step: 46, end_nodes: 3\n",
      "step: 47, end_nodes: 5\n",
      "step: 48, end_nodes: 5\n",
      "step: 49, end_nodes: 7\n",
      "step: 50, end_nodes: 8\n",
      "caption predict: SOMEONE and SOMEONE box her gaze remains carrie tears.\n",
      "\n",
      "video: ('ZOMBIELAND_DVS477.avi',)\n",
      "caption origin: They roll SOMEONE off the mansion balcony.\n",
      "step: 2, end_nodes: 0\n",
      "step: 3, end_nodes: 0\n",
      "step: 4, end_nodes: 0\n",
      "step: 5, end_nodes: 0\n",
      "step: 6, end_nodes: 0\n",
      "step: 7, end_nodes: 0\n",
      "step: 8, end_nodes: 0\n",
      "step: 9, end_nodes: 0\n",
      "step: 10, end_nodes: 1\n",
      "step: 11, end_nodes: 1\n",
      "step: 12, end_nodes: 1\n",
      "step: 13, end_nodes: 1\n",
      "step: 14, end_nodes: 2\n",
      "step: 15, end_nodes: 3\n",
      "step: 16, end_nodes: 4\n",
      "step: 17, end_nodes: 5\n",
      "step: 18, end_nodes: 5\n",
      "step: 19, end_nodes: 5\n",
      "step: 20, end_nodes: 5\n",
      "step: 21, end_nodes: 5\n",
      "step: 22, end_nodes: 5\n",
      "step: 23, end_nodes: 6\n",
      "step: 24, end_nodes: 7\n",
      "step: 25, end_nodes: 8\n",
      "step: 26, end_nodes: 9\n",
      "step: 27, end_nodes: 9\n",
      "caption predict: SOMEONE and SOMEONE look down sedan.\n",
      "\n",
      "video: ('THE_GIRL_WITH_THE_DRAGON_TATTOO_DVS1216.avi',)\n",
      "caption origin: Approaching the vehicle SOMEONE removes the gun from her waistband and holds it at her side.\n",
      "step: 2, end_nodes: 0\n",
      "step: 3, end_nodes: 0\n",
      "step: 4, end_nodes: 0\n",
      "step: 5, end_nodes: 0\n",
      "step: 6, end_nodes: 0\n",
      "step: 7, end_nodes: 0\n",
      "step: 8, end_nodes: 0\n",
      "step: 9, end_nodes: 0\n",
      "step: 10, end_nodes: 0\n",
      "step: 11, end_nodes: 0\n",
      "step: 12, end_nodes: 0\n",
      "step: 13, end_nodes: 0\n",
      "step: 14, end_nodes: 0\n",
      "step: 15, end_nodes: 0\n",
      "step: 16, end_nodes: 0\n",
      "step: 17, end_nodes: 0\n",
      "step: 18, end_nodes: 0\n",
      "step: 19, end_nodes: 0\n",
      "step: 20, end_nodes: 0\n",
      "step: 21, end_nodes: 0\n",
      "step: 22, end_nodes: 0\n",
      "step: 23, end_nodes: 0\n",
      "step: 24, end_nodes: 0\n",
      "step: 25, end_nodes: 0\n",
      "step: 26, end_nodes: 0\n",
      "step: 27, end_nodes: 0\n",
      "step: 28, end_nodes: 0\n",
      "step: 29, end_nodes: 0\n",
      "step: 30, end_nodes: 0\n",
      "step: 31, end_nodes: 0\n",
      "step: 32, end_nodes: 0\n",
      "step: 33, end_nodes: 0\n",
      "step: 34, end_nodes: 0\n",
      "step: 35, end_nodes: 0\n",
      "step: 36, end_nodes: 0\n",
      "step: 37, end_nodes: 0\n",
      "step: 38, end_nodes: 0\n",
      "step: 39, end_nodes: 0\n",
      "step: 40, end_nodes: 0\n",
      "step: 41, end_nodes: 0\n",
      "step: 42, end_nodes: 0\n",
      "step: 43, end_nodes: 0\n",
      "step: 44, end_nodes: 0\n",
      "step: 45, end_nodes: 0\n",
      "step: 46, end_nodes: 0\n",
      "step: 47, end_nodes: 0\n",
      "step: 48, end_nodes: 0\n",
      "step: 49, end_nodes: 0\n",
      "step: 50, end_nodes: 0\n",
      "50 no end node\n",
      "caption predict: Now SOMEONE and SOMEONE box her gaze father aims her aims aims her gun aims her gun aims aims aims aims her gun drag aims aims aims aims aims aims aims aims aims aims aims aims aims aims aims aims cradle gun. gun. gun. gun like\n",
      "\n",
      "video: ('PARENTAL_GUIDANCE_DVS272.avi',)\n",
      "caption origin: Missing them, SOMEONE plunges to the bottom of the ramp and lands hard in a sprawl.\n",
      "step: 2, end_nodes: 0\n",
      "step: 3, end_nodes: 0\n",
      "step: 4, end_nodes: 0\n",
      "step: 5, end_nodes: 0\n",
      "step: 6, end_nodes: 0\n",
      "step: 7, end_nodes: 0\n",
      "step: 8, end_nodes: 0\n",
      "step: 9, end_nodes: 0\n",
      "step: 10, end_nodes: 0\n",
      "step: 11, end_nodes: 0\n",
      "step: 12, end_nodes: 0\n",
      "step: 13, end_nodes: 0\n",
      "step: 14, end_nodes: 0\n",
      "step: 15, end_nodes: 0\n",
      "step: 16, end_nodes: 0\n",
      "step: 17, end_nodes: 0\n",
      "step: 18, end_nodes: 0\n",
      "step: 19, end_nodes: 0\n",
      "step: 20, end_nodes: 0\n",
      "step: 21, end_nodes: 0\n",
      "step: 22, end_nodes: 0\n",
      "step: 23, end_nodes: 0\n",
      "step: 24, end_nodes: 0\n",
      "step: 25, end_nodes: 0\n",
      "step: 26, end_nodes: 0\n",
      "step: 27, end_nodes: 0\n",
      "step: 28, end_nodes: 0\n",
      "step: 29, end_nodes: 0\n",
      "step: 30, end_nodes: 1\n",
      "step: 31, end_nodes: 1\n",
      "step: 32, end_nodes: 1\n",
      "step: 33, end_nodes: 1\n",
      "step: 34, end_nodes: 1\n",
      "step: 35, end_nodes: 1\n",
      "step: 36, end_nodes: 2\n",
      "step: 37, end_nodes: 3\n",
      "step: 38, end_nodes: 3\n",
      "step: 39, end_nodes: 3\n",
      "step: 40, end_nodes: 3\n",
      "step: 41, end_nodes: 3\n",
      "step: 42, end_nodes: 3\n",
      "step: 43, end_nodes: 3\n",
      "step: 44, end_nodes: 3\n",
      "step: 45, end_nodes: 3\n",
      "step: 46, end_nodes: 3\n",
      "step: 47, end_nodes: 5\n",
      "step: 48, end_nodes: 5\n",
      "step: 49, end_nodes: 7\n",
      "step: 50, end_nodes: 7\n",
      "caption predict: Now SOMEONE and SOMEONE box her dog SOMEONE her aims aims aims aims her gun and cradle aims aims aims aims her gun like descends gun. cradle aims aims her gun.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "captioner = BeamSearchCaptioner(model, sp, tar_max_seq_length, 3, 10, 5)\n",
    "captioner.caption_video_from_dataloader(dataloader, count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
